{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.kaggle.com/nroman/lgb-single-model-lb-0-9419\n",
    "> https://www.kaggle.com/roydatascience/light-gbm-with-complete-eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Please give your feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing necessary library**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'm on Ron's branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from time import time\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "warnings.simplefilter('ignore')\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../input/ieee-fraud-detection/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\")\n",
    "train_tr = pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id.shape, train_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = pd.read_csv(\"../input/ieee-fraud-detection/test_identity.csv\")\n",
    "test_tr = pd.read_csv(\"../input/ieee-fraud-detection/test_transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id.shape, test_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging transaction and Identity **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train_tr, train_id, on='TransactionID', how='left')\n",
    "test = pd.merge(test_tr, test_id, on='TransactionID', how='left')\n",
    "\n",
    "del test_id, test_tr\n",
    "del train_id, train_tr\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative Downsampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative downsampling\n",
    "train_pos = train[train['isFraud']==1]\n",
    "train_neg = train[train['isFraud']==0]\n",
    "\n",
    "train_neg = train_neg.sample(int(train_pos.shape[0] ), random_state=42)\n",
    "train = pd.concat([train_pos,train_neg]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos.shape, train_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 2*int(train_pos.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_pos\n",
    "del train_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From below we can see that there are a lot of features with almost 99% nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sorting features on basis of TransactionDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values('TransactionDT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taking all features**\n",
    "> Initially I will start with all the features and then will drop most of the features on the basis of count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_features = [col for col in train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From below we can see that length of features is 434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(useful_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Displaying all the columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train[\"isFraud\"]\n",
    "train.drop([\"isFraud\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatinating train and test as one dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Transaction_day_of_week'] = np.floor((train['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "test['Transaction_day_of_week'] = np.floor((test['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "train['Transaction_hour'] = np.floor(train['TransactionDT'] / 3600) % 24\n",
    "test['Transaction_hour'] = np.floor(test['TransactionDT'] / 3600) % 24\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Card feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['uid'] = train['card1'].astype(str)+'_'+train['card2'].astype(str)\n",
    "test['uid'] = test['card1'].astype(str)+'_'+test['card2'].astype(str)\n",
    "\n",
    "train['uid1'] = train['uid'].astype(str)+'_'+train['card3'].astype(str)\n",
    "test['uid1'] = test['uid'].astype(str)+'_'+test['card3'].astype(str)\n",
    "\n",
    "\n",
    "train['uid2'] = train['uid'].astype(str)+'_'+train['card3'].astype(str)+'_'+train['card5'].astype(str)\n",
    "test['uid2'] = test['uid'].astype(str)+'_'+test['card3'].astype(str)+'_'+test['card5'].astype(str)\n",
    "\n",
    "train['uid3'] = train['uid2'].astype(str)+'_'+train['addr1'].astype(str)+'_'+train['addr2'].astype(str)\n",
    "test['uid3'] = test['uid2'].astype(str)+'_'+test['addr1'].astype(str)+'_'+test['addr2'].astype(str)\n",
    "\n",
    "train['uid4'] = train['card4'].astype(str)+'_'+train['card6'].astype(str)\n",
    "test['uid4'] = test['card4'].astype(str)+'_'+test['card6'].astype(str)\n",
    "\n",
    "train['TransactionAmt_check'] = np.where(train['TransactionAmt'].isin(test['TransactionAmt']), 1, 0)\n",
    "test['TransactionAmt_check']  = np.where(test['TransactionAmt'].isin(train['TransactionAmt']), 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Id Feaures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['id'] = train['id_35'].astype(str)+'_'+train['id_36'].astype(str)\n",
    "test['id'] = test['id_35'].astype(str)+'_'+test['id_36'].astype(str)\n",
    "\n",
    "train['id1'] = train['id'].astype(str)+'_'+train['id_37'].astype(str)\n",
    "test['id1'] = test['id'].astype(str)+'_'+test['id_37'].astype(str)\n",
    "\n",
    "\n",
    "train['id2'] = train['id1'].astype(str)+'_'+train['id_38'].astype(str)\n",
    "test['id2'] = test['id1'].astype(str)+'_'+test['id_38'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop([\"TransactionID\", \"TransactionDT\"], axis=1, inplace=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here I will treat all features as categorical except TransationAmt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neglect = [\"TransactionAmt\", 'Transaction_day_of_week', 'Transaction_hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_features = [col for col in train.columns if col not in neglect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['M_na'] = abs(train.isna().sum(axis=1).astype(np.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nan = [\"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\", \"C8\", \"C9\", \"C10\", \"C11\", \"C12\", \"C13\", \"C14\",\\\n",
    "          \"D1\", \"M_na\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"V96\", \"V97\", \"V98\", \"V99\", \"V100\", \"V101\", \"V102\", \"V103\", \"V104\", \"V105\", \"V106\",\\\n",
    "#          \"V107\", \"V108\", \"V109\", \"V110\", \"V111\", \"V112\", \"V113\", \"V114\", \"V115\", \"V116\", \"V117\",\"V118\",\\\n",
    " #         \"V119\", \"V120\", \"V121\",\"V122\",\"V123\", \"V124\", \"V125\", \"V126\", \"V127\", \"V128\", \"V129\", \"V130\",\\\n",
    "  #        \"V131\", \"V132\", \"V133\", \"V134\", \"V135\", \"V136\", \"V137\", \"V297\", \"V298\", \"V299\", \"V300\",\\\n",
    "   #       \"V301\", \"V301\", \"V302\", \"V303\", \"V304\", \"V305\", \"V306\", \"V307\", \"V308\", \"V309\", \"V310\",\\\n",
    "    #      \"V311\", \"V312\", \"V313\", \"V314\", \"V315\", \"V316\", \"V317\", \"V318\", \"V319\", \"V320\", \"V321\",\\\n",
    "     #     \"V279\", \"V280\", \"V281\", \"V282\", \"V283\", \"V284\", \"V285\", \"V286\", \"V287\", \"V288\", \"V289\", \"V290\",\\\n",
    "      #    \"V291\", \"V292\", \"V293\", \"V294\", \"V295\", \"V296\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This block of code count every features and drop original features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0        \n",
    "for feature in useful_features:\n",
    "    \n",
    "        # Count encoded separately for train and test\n",
    "    train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n",
    "    if feature not in non_nan:\n",
    "        train.drop([feature], axis=1,inplace=True)\n",
    "    print(\"Done\" + str(i))\n",
    "    i+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping below features as these seems to be repeating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropping =[\"D8_count_dist\", \"V138_count_dist\", \"V139_count_dist\", \"V140_count_dist\", \"V141_count_dist\",\\\n",
    "           \"V146_count_dist\", \"V147_count_dist\", \"V148_count_dist\", \"V149_count_dist\", \"V144_count_dist\",\\\n",
    "           \"V145_count_dist\", \"V150_count_dist\", \"V151_count_dist\", \"V152_count_dist\", \"V153_count_dist\",\\\n",
    "           \"V154_count_dist\", \"V155_count_dist\", \"V156_count_dist\", \"V157_count_dist\", \"V158_count_dist\",\\\n",
    "           \"V159_count_dist\", \"V160_count_dist\", \"V161_count_dist\", \"V162_count_dist\", \"V163_count_dist\",\\\n",
    "           \"V164_count_dist\", \"V165_count_dist\", \"V166_count_dist\", \"V168_count_dist\", \"V170_count_dist\",\\\n",
    "           \"V171_count_dist\", \"V172_count_dist\", \"V173_count_dist\", \"V174_count_dist\", \"V175_count_dist\",\\\n",
    "           \"V176_count_dist\", \"V177_count_dist\", \"V178_count_dist\", \"V179_count_dist\", \"V180_count_dist\",\\\n",
    "           \"V181_count_dist\", \"V182_count_dist\", \"V183_count_dist\", \"V184_count_dist\", \"V185_count_dist\",\\\n",
    "           \"V186_count_dist\", \"V187_count_dist\", \"V188_count_dist\", \"V189_count_dist\", \"V190_count_dist\",\\\n",
    "           \"V191_count_dist\", \"V192_count_dist\", \"V193_count_dist\", \"V194_count_dist\", \"V195_count_dist\",\\\n",
    "           \"V196_count_dist\", \"V197_count_dist\", \"V198_count_dist\", \"V199_count_dist\", \"V200_count_dist\",\\\n",
    "           \"V201_count_dist\", \"V202_count_dist\", \"V203_count_dist\", \"V204_count_dist\", \"V205_count_dist\",\\\n",
    "           \"V206_count_dist\", \"V207_count_dist\", \"V208_count_dist\", \"V209_count_dist\", \"V210_count_dist\",\\\n",
    "           \"V211_count_dist\", \"V212_count_dist\", \"V213_count_dist\", \"V214_count_dist\", \"V215_count_dist\",\\\n",
    "           \"V216_count_dist\", \"V218_count_dist\", \"V219_count_dist\", \"V221_count_dist\", \"V222_count_dist\",\\\n",
    "           \"V223_count_dist\", \"V224_count_dist\", \"V225_count_dist\", \"V226_count_dist\", \"V227_count_dist\",\\\n",
    "           \"V228_count_dist\", \"V229_count_dist\", \"V230_count_dist\", \"V231_count_dist\", \"V232_count_dist\",\\\n",
    "           \"V233_count_dist\", \"V234_count_dist\", \"V235_count_dist\", \"V236_count_dist\", \"V237_count_dist\",\\\n",
    "           \"V205_count_dist\", \"V205_count_dist\", \"V205_count_dist\", \"V205_count_dist\", \"V205_count_dist\",\\\n",
    "           \"V238_count_dist\", \"V239_count_dist\", \"V240_count_dist\", \"V241_count_dist\", \"V242_count_dist\",\\\n",
    "           \"V243_count_dist\", \"V244_count_dist\", \"V245_count_dist\",\"V246_count_dist\", \"V247_count_dist\",\\\n",
    "           \"V248_count_dist\", \"V249_count_dist\", \"V250_count_dist\", \"V251_count_dist\", \"V252_count_dist\",\\\n",
    "           \"V253_count_dist\", \"V254_count_dist\", \"V255_count_dist\", \"V256_count_dist\", \"V257_count_dist\",\\\n",
    "           \"V258_count_dist\", \"V259_count_dist\", \"V260_count_dist\", \"V261_count_dist\", \"V262_count_dist\",\\\n",
    "           \"V263_count_dist\", \"V264_count_dist\", \"V265_count_dist\", \"V266_count_dist\", \"V267_count_dist\",\\\n",
    "           \"V268_count_dist\", \"V269_count_dist\", \"V270_count_dist\", \"V271_count_dist\", \"V272_count_dist\",\\\n",
    "           \"V273_count_dist\", \"V274_count_dist\", \"V275_count_dist\", \"V276_count_dist\", \"V277_count_dist\",\\\n",
    "           \"V278_count_dist\", \"V323_count_dist\", \"V324_count_dist\", \"V325_count_dist\", \"V326_count_dist\",\\\n",
    "           \"V327_count_dist\", \"V328_count_dist\", \"V329_count_dist\", \"V330_count_dist\", \"V331_count_dist\",\\\n",
    "           \"V332_count_dist\", \"V333_count_dist\", \"V334_count_dist\", \"V335_count_dist\", \"V336_count_dist\",\\\n",
    "           \"V237_count_dist\", \"V238_count_dist\", \"V239_count_dist\", \"id_04_count_dist\", \"id_06_count_dist\",\\\n",
    "           \"id_08_count_dist\", \"id_10_count_dist\", \"id_22_count_dist\", \"id_27_count_dist\", \"id_29_count_dist\",\\\n",
    "           \"id_36_count_dist\", \"id_37_count_dist\", \"id_38_count_dist\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(dropping, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dropping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful_features = [col for col in train.columns]\n",
    "#for feature in useful_features:\n",
    "    \n",
    "        # Count encoded separately for train and test\n",
    "#    train[feature] = np.log(train[feature])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Below we can see that all I am left with is count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1)#\n",
    "#y = train.sort_values('TransactionDT')['isFraud']\n",
    "#test = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del train\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Again seperating data into train and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.iloc[:l, :]\n",
    "test = train.iloc[l:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train test and split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Set\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=0.20, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Lightgbm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "categorical_var = np.where(train.dtypes != np.float)[0]\n",
    "print('\\nCategorical Variables indices : ',categorical_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 491,\n",
    "          'min_child_weight': 0.03454472573214212,\n",
    "          'feature_fraction': 0.3797454081646243,\n",
    "          'bagging_fraction': 0.4181193142567742,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.006883242363721497,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.3899927210061127,\n",
    "          'reg_lambda': 0.6485237330340494,\n",
    "          'random_state': 47,\n",
    "          \"n_jobs\" : -1\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "aucs = list()\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = X.columns\n",
    "\n",
    "training_start_time = time()\n",
    "for fold, (trn_idx, test_idx) in enumerate(folds.split(X, y)):\n",
    "    start_time = time()\n",
    "    print('Training on fold {}'.format(fold + 1))\n",
    "    \n",
    "    trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n",
    "    clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n",
    "    \n",
    "    feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n",
    "    aucs.append(clf.best_score['valid_1']['auc'])\n",
    "    \n",
    "    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n",
    "print('-' * 30)\n",
    "print('Training has finished.')\n",
    "print('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n",
    "print('Mean AUC:', np.mean(aucs))\n",
    "print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances['average'] = feature_importances[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\n",
    "feature_importances.to_csv('feature_importances.csv')\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\n",
    "plt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf right now is the last model, trained with 80% of data and validated with 20%\n",
    "best_iter = clf.best_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(**params, num_boost_round=best_iter)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['isFraud'] = clf.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('ieee_cis_fraud_detection_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> thank you all please let me know where did I go wrong.\n",
    "> Thankyou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
